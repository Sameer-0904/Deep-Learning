# Deep Learning â€“ Conceptual & Practical Notebooks ğŸ§ âœ¨

This repository contains a collection of **Jupyter Notebook implementations** focused on understanding **Deep Learning fundamentals** through theory, visualization, and hands-on coding.  
Most notebooks emphasize **from-scratch implementations** to strengthen conceptual clarity.

---

## ğŸ“ Repository Structure

Deep-Learning/
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ 01_Perceptron_Code_Example.ipynb
â”œâ”€â”€ 02_Perceptron_Training.ipynb
â”œâ”€â”€ 03_Perceptron_Loss_Function.ipynb
â”œâ”€â”€ 04_Graduate_Admissions_Deep_Learning.ipynb
â”œâ”€â”€ 05_MNIST_Deep_Learning.ipynb
â”œâ”€â”€ 06_BackProp_without_Keras_Lib.ipynb
â”œâ”€â”€ 07_Memoization_Demo.ipynb
â”œâ”€â”€ 08_Vanishing_Gradient.ipynb
â”œâ”€â”€ 09_Early_Stopping.ipynb
â”œâ”€â”€ 10_Feature_Scaling.ipynb
â”œâ”€â”€ 11_Dropout_Classification_Example.ipynb
â”œâ”€â”€ 12_Dropout_Regression.ipynb
â””â”€â”€ README.md


---

## ğŸ“˜ Notebook Descriptions

### 01. Perceptron Code Example
Introduces the **basic perceptron model**, explaining how a single neuron works with inputs, weights, bias, and activation.

---

### 02. Perceptron Training
Demonstrates **training a perceptron**, including weight updates, learning rate usage, and decision boundary formation.

---

### 03. Perceptron Loss Function
Explains different **loss functions** used in perceptron and neural network training and how loss affects learning.

---

### 04. Graduate Admissions â€“ Deep Learning
Applies deep learning concepts to a **real-world regression problem** predicting graduate admission chances.

---

### 05. MNIST Deep Learning
Implements a neural network for **handwritten digit classification** using the MNIST dataset.

---

### 06. Backpropagation Without Keras
A **from-scratch implementation of backpropagation**, showing gradient calculation and weight updates without high-level libraries.

---

### 07. Memoization Demo
Demonstrates **memoization** and its importance in optimizing repeated computations during training.

---

### 08. Vanishing Gradient
Explains the **vanishing gradient problem**, its causes, and how it affects deep neural networks.

---

### 09. Early Stopping
Illustrates **early stopping** as a regularization technique to prevent overfitting during training.

---

### 10. Feature Scaling
Covers **feature scaling techniques** such as normalization and standardization and their impact on model performance.

---

### 11. Dropout â€“ Classification Example
Demonstrates **dropout regularization** applied to a classification problem to improve generalization.

---

### 12. Dropout â€“ Regression
Shows how **dropout can be applied in regression models** and its effect on overfitting.

---

## ğŸ› ï¸ Requirements

- Python 3.x
- Jupyter Notebook
- NumPy
- Pandas
- Matplotlib
- Tensorflow
- Keras

---

## ğŸ¯ Purpose of This Repository

- Strengthen **deep learning fundamentals**
- Understand **neural networks from scratch**
- Learn **regularization and optimization techniques**
- Practice real-world **classification and regression problems**

---

## ğŸ“Œ Author

**Sameer Prajapati**  
AI â€¢ ML â€¢ Deep Learning Enthusiast ğŸš€

---

â­ If you find this repository helpful, donâ€™t forget to star it!
