# Deep Learning ‚Äì Conceptual & Practical Notebooks üß†‚ú®

This repository contains a collection of **Jupyter Notebook implementations** focused on understanding **Deep Learning fundamentals** through theory, visualization, and hands-on coding.  
Most notebooks emphasize **from-scratch implementations** to strengthen conceptual clarity.

---

## üìò Notebook Descriptions

### 01. Perceptron Code Example
Introduces the **basic perceptron model**, explaining how a single neuron works with inputs, weights, bias, and activation.

---

### 02. Perceptron Training
Demonstrates **training a perceptron**, including weight updates, learning rate usage, and decision boundary formation.

---

### 03. Perceptron Loss Function
Explains different **loss functions** used in perceptron and neural network training and how loss affects learning.

---

### 04. Graduate Admissions ‚Äì Deep Learning
Applies deep learning concepts to a **real-world regression problem** predicting graduate admission chances.

---

### 05. MNIST Deep Learning
Implements a neural network for **handwritten digit classification** using the MNIST dataset.

---

### 06. Backpropagation Without Keras
A **from-scratch implementation of backpropagation**, showing gradient calculation and weight updates without high-level libraries.

---

### 07. Memoization Demo
Demonstrates **memoization** and its importance in optimizing repeated computations during training.

---

### 08. Vanishing Gradient
Explains the **vanishing gradient problem**, its causes, and how it affects deep neural networks.

---

### 09. Early Stopping
Illustrates **early stopping** as a regularization technique to prevent overfitting during training.

---

### 10. Feature Scaling
Covers **feature scaling techniques** such as normalization and standardization and their impact on model performance.

---

### 11. Dropout ‚Äì Classification Example
Demonstrates **dropout regularization** applied to a classification problem to improve generalization.

---

### 12. Dropout ‚Äì Regression
Shows how **dropout can be applied in regression models** and its effect on overfitting.

---

### 13. Regularization ‚Äì L2 (Weight Decay)
Explains **L2 regularization (Weight Decay)**, how it penalizes large weights, and how it helps reduce overfitting in neural networks.

---

### 14. Zero Initialization Problem ‚Äì ReLU Activation
This notebook explains the **zero initialization problem** when using **ReLU activation functions**.  
It demonstrates how initializing all weights to zero causes neurons to learn identical features, leading to **symmetry problems** and ineffective training.

---

### 15. Zero Initialization Problem ‚Äì Sigmoid Activation
This notebook explores the **zero initialization issue with Sigmoid activation**.  
It highlights how zero initialization leads to **poor gradient flow**, slow learning, and symmetry, making it unsuitable for training deep neural networks.

---

### 16. Xavier and He Weight Initialization
This notebook explains **Xavier (Glorot) and He initialization techniques**, focusing on how proper weight initialization helps maintain stable variance of activations and gradients across layers, leading to faster convergence and better training of deep neural networks.

---

### 17. Batch Normalization
This notebook covers **Batch Normalization**, a technique used to normalize layer inputs during training.  
It demonstrates how batch normalization improves training stability, accelerates convergence, and reduces sensitivity to weight initialization.

---

### 18. EWMA Optimizer (Exponentially Weighted Moving Average)
This notebook explains the **EWMA (Exponentially Weighted Moving Average) optimizer**, showing how it smooths gradients by maintaining an exponentially decaying average of past gradients, helping reduce noise, stabilize updates, and improve convergence during neural network training.

---


## üõ†Ô∏è Requirements

- Python 3.x
- Jupyter Notebook
- NumPy
- Pandas
- Matplotlib
- Tensorflow
- Keras

---

## üéØ Purpose of This Repository

- Strengthen **deep learning fundamentals**
- Understand **neural networks from scratch**
- Learn **regularization and optimization techniques**
- Practice real-world **classification and regression problems**

---

## üìå Author

**Sameer Prajapati**  
AI ‚Ä¢ ML ‚Ä¢ Deep Learning Enthusiast üöÄ

---

‚≠ê If you find this repository helpful, don‚Äôt forget to star it!
